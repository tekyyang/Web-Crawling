__author__ = 'yyb'

from bs4 import BeautifulSoup
import re
import requests
import time
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import csv
import json
import pandas as pd
import os
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# this file is for data collection
# functions include user's followers' list crawling, account's active followers crawling and user's tweets crawling


#get an account's followers list
def follower_crawl(username,savepath):

    success_num=0
    fail_num=0

    username=str(username)
    browser=webdriver.Firefox()
    browser.implicitly_wait(10)

    browser.get("https://twitter.com/"+username+"/followers")
    time.sleep(5)

    name=browser.find_element_by_css_selector('input.js-initial-focus')#div.LoginForm-input LoginForm-username #input.js-signin-email
    name.send_keys("your phone number")

    password=browser.find_element_by_css_selector('input.js-password-field')
    password.send_keys("password")

    time.sleep(2)
    login_button=browser.find_element_by_xpath("//button[@type='submit']")#this place use xpath
    login_button.click()

    parameter_list=browser.find_elements_by_xpath("//span[@class='ProfileNav-value']")

    #try:
        #scroll_num=int(int(follower_num)/2+2)
    #except:
        #follower_num=re.sub(r',','',follower_num)
        #scroll_num=int(float(follower_num)/2+2)
    scroll_num=200

    load=browser.find_element_by_css_selector('body')#text-input email-input js-signin-email
    for i in range(scroll_num):
        load.send_keys(Keys.PAGE_DOWN)
        time.sleep(0.2)

    follower_list_for_one_user=[]

    scrnames=browser.find_elements_by_css_selector('span.ProfileCard-screenname')#b.u-linkComplex-target
    for scrname in scrnames:#get followers for each username
        a=scrname.text
        a=a[1:]
        a=a.split(" ")[0]
        follower_list_for_one_user.append(a)

        # save each result to file
        with open(savepath + str(username) + ".csv", 'a') as csv:
            csv.write(a+'\n')

    browser.close()

# use the later one with bs4
def user_page_crawl(username,savepath):
    username=str(username)
    browser=webdriver.Firefox()
    browser.implicitly_wait(2)

    #url="https://twitter.com/CU_FASS/following"
    browser.get("https://twitter.com/"+username+'?lang=en')
    time.sleep(2)

    name=browser.find_element_by_name('session[username_or_email]')
    name.send_keys("3437777135")

    password=browser.find_element_by_name('session[password]')
    password.send_keys("yyb901027")

    time.sleep(2)

    login_button=browser.find_element_by_xpath("/html/body/div[2]/div[1]/div/div/div/div/ul[2]/li/div/div[3]/form/input[1]")#this place use xpath
    login_button.click()
    time.sleep(2)

    html=browser.page_source
    soup=BeautifulSoup(html,"html.parser")

    #tweet_num_find=soup.find_all('a',attrs={'class':'ProfileNav-stat ProfileNav-stat--link u-borderUserColor u-textCenter js-tooltip js-nav'})
    #text=str(tweet_num_find[0])
    #tweet_num=re.search(r'<span class="ProfileNav-value" data-count="(.+)" data-is-compact="false">',text).group(1)
    tweet_num = 1000

    load = browser.find_element_by_css_selector('body')
    for i in range(int(tweet_num)):
        load.send_keys(Keys.PAGE_DOWN)  # scroll to the downside of the page
        time.sleep(0.2)


    html=browser.page_source
    soup=BeautifulSoup(html,"html.parser") # the updated page
    #print(soup)

    timestamps=soup.find_all('a',attrs={'class':"tweet-timestamp js-permalink js-nav js-tooltip"}) #return bs4 tag
    timestamps=[time['title'] for time in timestamps]
    times=[re.search(r'(.*) - >*',time).group(1) for time in timestamps]
    dates=[re.search(r'- (.*)',time).group(1) for time in timestamps]
    print(len(times))
    print(len(dates))

    soup=soup.getText
    soup=str(soup)
    tweets=re.findall(r'<p class="TweetTextSize TweetTextSize--normal js-tweet-text tweet-text" data-aria-label-part="0" lang="en">(.+)</p>',soup)
    tweets=[re.sub(r'<(.*?)>','',tweet) for tweet in tweets]

    content_list=[]
    #print(len(tweets))
    for tweet in tweets:
        print(tweet)
    with open(savepath+username+'.txt', 'a') as txt:
        for tweet in tweets:
            tweet=tweet+'\n'
            txt.write(tweet)


# get an account's active user's list
def get_active_user(brandname,savepath):

    browser=webdriver.Firefox()
    browser.implicitly_wait(2)
    browser.get("https://twitter.com/"+brandname+'?lang=en')
    time.sleep(2)

    name=browser.find_element_by_name('session[username_or_email]')
    name.send_keys("3437777135")

    password=browser.find_element_by_name('session[password]')
    password.send_keys("yyb901027")
    time.sleep(2)

    login_button=browser.find_element_by_xpath("/html/body/div[2]/div[1]/div/div/div/div/ul[2]/li/div/div[3]/form/input[1]")#this place use xpath
    login_button.click()
    time.sleep(2)

    html=browser.page_source
    soup=BeautifulSoup(html,"html.parser")

    tweet_num_find=soup.find_all('a',attrs={'class':'ProfileNav-stat ProfileNav-stat--link u-borderUserColor u-textCenter js-tooltip js-nav'})
    text=str(tweet_num_find[0])
    tweet_num=re.search(r'<span class="ProfileNav-value" data-count="(.+)" data-is-compact="false">',text).group(1)
    print('Tweet numbers: '+str(tweet_num))

    load = browser.find_element_by_css_selector('body')
    for i in range(int(tweet_num)):
        load.send_keys(Keys.PAGE_DOWN)  # scroll to the downside of the page
        time.sleep(0.2)

    html = browser.page_source
    soup = BeautifulSoup(html, "html.parser")  # the updated page


    tweet_links_location=soup.find_all('a',attrs={'class':'tweet-timestamp js-permalink js-nav js-tooltip'})
    tweet_links=['https://twitter.com'+link['href'] for link in tweet_links_location]
    print('Tweet links: '+str(len(tweet_links)))

    active_user_screen_names=[]
    count=0

    for link in tweet_links:
        #print(link)
        per_tweet_content = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})
        per_tweet_content = per_tweet_content.text
        screen_names=re.findall(r'data-screen-name="(.*?)"',per_tweet_content) # get the names directly
        active_user_screen_names+=screen_names
        count+=1
        sys.stdout.write('Processing the '+str(count)+ ' link now!' + '\r')

    active_user_screen_names=list(set(active_user_screen_names))

    with open(savepath+brandname+'.csv', 'a') as csv:
        for name in active_user_screen_names:
            name=name+'\n'
            csv.write(name)


#get a user's tweets
def user_page_crawl_by_bs4(username,savepath):
    import time
    username=str(username)
    browser=webdriver.Firefox()
    browser.implicitly_wait(2)

    #url="https://twitter.com/CU_FASS/following"
    browser.get("https://twitter.com/"+username+'?lang=en')
    time.sleep(2)

    name=browser.find_element_by_name('session[username_or_email]')
    name.send_keys("3437777135")

    password=browser.find_element_by_name('session[password]')
    password.send_keys("yyb901027")

    time.sleep(2)

    login_button=browser.find_element_by_xpath("/html/body/div[2]/div[1]/div/div/div/div/ul[2]/li/div/div[3]/form/input[1]")#this place use xpath
    login_button.click()
    time.sleep(2)

    html = browser.page_source
    soup = BeautifulSoup(html, "html.parser")
    try:
        tweet_num_find = soup.find_all('a', attrs={
            'class': 'ProfileNav-stat ProfileNav-stat--link u-borderUserColor u-textCenter js-tooltip js-nav'})
        text = str(tweet_num_find[0])
        tweet_num = re.search(r'<span class="ProfileNav-value" data-count="(.+)" data-is-compact="false">', text).group(1)
    except:
        tweet_num = 5000

    load = browser.find_element_by_css_selector('body')
    for i in range(int(tweet_num)):
        load.send_keys(Keys.PAGE_DOWN)  # scroll to the downside of the page
        time.sleep(0.2)


    html=browser.page_source
    soup=BeautifulSoup(html,"html.parser") # the updated page
    soup=soup.find('div','stream')
    #print(soup)
    content=soup.find_all('li',attrs={'class':'js-stream-item stream-item stream-item '})
    for each_tweet in content:
        each_tweet=str(each_tweet)
        try:
            tweet=re.search(r'<p class="TweetTextSize TweetTextSize--normal js-tweet-text tweet-text" data-aria-label-part="0" lang="en">(.+)</p>',each_tweet).group(1)
            tweet=re.sub(r'<.*?>','',tweet)
            tweet=re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+','',tweet)
            tweet=re.sub(r'pic.twitter.com/[a-zA-Z]+','',tweet)

        except:
            tweet=''

        try:
            timestamp=re.search(r'title="(.*?)"><span class="_timestamp js-short-timestamp "',each_tweet).group(1)
            time = re.search(r'(.*) - >*', timestamp).group(1)
            date = re.search(r'- (.*)', timestamp).group(1)
        except:
            timestamp=''
            time=''
            date=''

        with open(savepath+username+'.txt', 'a') as text_file:
            line=tweet+'^'+time+'^'+date+'\n'
            text_file.write(line)
    print('finish '+username)
    browser.close()

def search_results(key_word,savepath):
    import time
    username = str(key)
    browser = webdriver.Firefox()
    browser.implicitly_wait(2)

    # url="https://twitter.com/CU_FASS/following"
    browser.get('https://twitter.com/search?q=%23'+key_word+'&src=typd')
    time.sleep(2)

    name = browser.find_element_by_name('session[username_or_email]')
    name.send_keys("3437777135")

    password = browser.find_element_by_name('session[password]')
    password.send_keys("yyb901027")

    time.sleep(2)

    login_button = browser.find_element_by_xpath(
        "/html/body/div[2]/div[1]/div/div/div/div/ul[2]/li/div/div[3]/form/input[1]")  # this place use xpath
    login_button.click()
    time.sleep(2)

    html = browser.page_source
    soup = BeautifulSoup(html, "html.parser")
    try:
        tweet_num_find = soup.find_all('a', attrs={
            'class': 'ProfileNav-stat ProfileNav-stat--link u-borderUserColor u-textCenter js-tooltip js-nav'})
        text = str(tweet_num_find[0])
        tweet_num = re.search(r'<span class="ProfileNav-value" data-count="(.+)" data-is-compact="false">', text).group(
            1)
    except:
        tweet_num = 5000

    load = browser.find_element_by_css_selector('body')
    for i in range(int(tweet_num)):
        load.send_keys(Keys.PAGE_DOWN)  # scroll to the downside of the page
        time.sleep(0.2)

    html = browser.page_source
    soup = BeautifulSoup(html, "html.parser")  # the updated page
    soup = soup.find('div', 'stream')
    # print(soup)
    content = soup.find_all('li', attrs={'class': 'js-stream-item stream-item stream-item '})
    for each_tweet in content:
        each_tweet = str(each_tweet)
        try:
            tweet = re.search(
                r'<p class="TweetTextSize TweetTextSize--normal js-tweet-text tweet-text" data-aria-label-part="0" lang="en">(.+)</p>',
                each_tweet).group(1)
            tweet = re.sub(r'<.*?>', '', tweet)
            tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+', '', tweet)
            tweet = re.sub(r'pic.twitter.com/[a-zA-Z]+', '', tweet)

        except:
            tweet = ''

        try:
            timestamp = re.search(r'title="(.*?)"><span class="_timestamp js-short-timestamp "', each_tweet).group(1)
            time = re.search(r'(.*) - >*', timestamp).group(1)
            date = re.search(r'- (.*)', timestamp).group(1)
        except:
            timestamp = ''
            time = ''
            date = ''

        with open(savepath + username + '.txt', 'a') as text_file:
            line = tweet + '^' + time + '^' + date + '\n'
            text_file.write(line)
    print('finish ' + username)
    browser.close()




# 1) get the content of positive and negative tweets from accounts---------
def get_P_N_followers_list(crawling_list,P_or_N):
    if P_or_N == 'P':
        for name in crawling_list:
            user_page_crawl_by_bs4(name,'/Users/yyb/Documents/5920_Projects/PositiveDataset/')
    else:
        for name in crawling_list:
            user_page_crawl_by_bs4(name,'/Users/yyb/Documents/5920_Projects/NegativeDataset/')


# these are the brands names
'''
positive_list=['samsungsg']
# finished: 'ilovedealssg','hungrygowhere','joannepeh','kiasuparents','MOEsg',
negative_list=[ 'tocsg','SGnews','sgdrivers','premierleague','belindaang','mtvasia','tiongbahruplaza',]
#main(positive_list,'P')
get_P_N_followers_list(negative_list,'N')
'''

# 2) get samsungsg's followers' list (not all) -----------------
#follower_crawl('samsungsg','/Users/yyb/Documents/5920_Projects/followers_of_positive/')


# 3) get samsungsg's followers' tweets ------------------
def get_P_followers_tweets(file_path):
    import csv
    with open(file_path,newline='') as file:
        follower_names = csv.reader(file)
        for name in follower_names:
            name=name[0]
            try:
                user_page_crawl_by_bs4(name,'/Users/yyb/Documents/5920_Projects/followers_tweets/')
                with open('/Users/yyb/Documents/5920_Projects/followers_of_positive/success_list.csv','a') as csv:
                    name=name+'\n'
                    csv.write(name)
            except:
                with open('/Users/yyb/Documents/5920_Projects/followers_of_positive/fail_list.csv','a') as csv:
                    name=name+'\n'
                    csv.write(name)

#get_P_followers_tweets('/Users/yyb/Documents/5920_Projects/followers_of_positive/samsungsg.csv')

#'JoannePeh',
waiting_list=['Camemberu','premierleague','sgbroadcast','mtvasia','SGAG_SG']
for name in waiting_list:
    user_page_crawl_by_bs4(name,'/Users/yyb/Documents/5920_Projects/New_data_collection_0312/Negative/')
